{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-eyJzE",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OpenRouterComponent-j1wSC",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-eyJzE{œdataTypeœ:œChatInputœ,œidœ:œChatInput-eyJzEœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-OpenRouterComponent-j1wSC{œfieldNameœ:œinput_valueœ,œidœ:œOpenRouterComponent-j1wSCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-eyJzE",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-eyJzEœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OpenRouterComponent-j1wSC",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOpenRouterComponent-j1wSCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-qpTOk",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "OpenRouterComponent-j1wSC",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Prompt-qpTOk{œdataTypeœ:œPromptœ,œidœ:œPrompt-qpTOkœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenRouterComponent-j1wSC{œfieldNameœ:œsystem_messageœ,œidœ:œOpenRouterComponent-j1wSCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-qpTOk",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-qpTOkœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OpenRouterComponent-j1wSC",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œOpenRouterComponent-j1wSCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "OpenRouterComponent",
            "id": "OpenRouterComponent-j1wSC",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-lrulI",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__OpenRouterComponent-j1wSC{œdataTypeœ:œOpenRouterComponentœ,œidœ:œOpenRouterComponent-j1wSCœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-lrulI{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-lrulIœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "OpenRouterComponent-j1wSC",
        "sourceHandle": "{œdataTypeœ:œOpenRouterComponentœ,œidœ:œOpenRouterComponent-j1wSCœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-lrulI",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-lrulIœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "description": "Get chat inputs from the Playground.",
          "display_name": "Chat Input",
          "id": "ChatInput-eyJzE",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "store_message",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "7a26c54d89ed",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 1
              },
              "module": "lfx.components.input_output.chat.ChatInput"
            },
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chat Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from lfx.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom lfx.base.io.chat import ChatComponent\nfrom lfx.inputs.inputs import BoolInput\nfrom lfx.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom lfx.schema.message import Message\nfrom lfx.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    documentation: str = \"https://docs.langflow.org/chat-input-and-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        # Ensure files is a list and filter out empty/None values\n        files = self.files if self.files else []\n        if files and not isinstance(files, list):\n            files = [files]\n        # Filter out None/empty values\n        files = [f for f in files if f is not None and f != \"\"]\n\n        session_id = self.session_id or self.graph.session_id or \"\"\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=session_id,\n            context_id=self.context_id,\n            files=files,\n        )\n        if session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "files": {
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "csv",
                  "json",
                  "pdf",
                  "txt",
                  "md",
                  "mdx",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "name": "files",
                "placeholder": "",
                "required": false,
                "show": true,
                "temp_file": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "advanced": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Hello"
              },
              "sender": {
                "advanced": true,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            }
          },
          "selected_output": "message",
          "type": "ChatInput"
        },
        "dragging": false,
        "height": 234,
        "id": "ChatInput-eyJzE",
        "measured": {
          "height": 234,
          "width": 320
        },
        "position": {
          "x": 683.3271781290201,
          "y": 786.3294994926094
        },
        "positionAbsolute": {
          "x": 689.5720422421635,
          "y": 765.155834131403
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-qpTOk",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": []
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "field_order": [
              "template"
            ],
            "frozen": false,
            "icon": "braces",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "3bf0b511e227",
              "module": "langflow.components.prompts.prompt.PromptComponent"
            },
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "method": "build_prompt",
                "name": "prompt",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "_input_type": "PromptInput",
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh."
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "prompt",
          "type": "Prompt"
        },
        "dragging": false,
        "height": 260,
        "id": "Prompt-qpTOk",
        "measured": {
          "height": 260,
          "width": 320
        },
        "position": {
          "x": 686.1163504192427,
          "y": 1047.0696070999504
        },
        "positionAbsolute": {
          "x": 690.2015147036818,
          "y": 1018.5443911764344
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "id": "ChatOutput-lrulI",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "8c87e536cca4",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "orjson",
                    "version": "3.10.15"
                  },
                  {
                    "name": "fastapi",
                    "version": "0.128.0"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 3
              },
              "module": "lfx.components.input_output.chat_output.ChatOutput"
            },
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean data before converting to string.",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom lfx.base.io.chat import ChatComponent\nfrom lfx.helpers.data import safe_convert\nfrom lfx.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom lfx.schema.data import Data\nfrom lfx.schema.dataframe import DataFrame\nfrom lfx.schema.message import Message\nfrom lfx.schema.properties import Source\nfrom lfx.template.field.base import Output\nfrom lfx.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/chat-input-and-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            advanced=True,\n            info=\"Whether to clean data before converting to string.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message) and not self.is_connected_to_chat_input():\n            message = self.input_value\n            # Update message properties\n            message.text = text\n            # Preserve existing session_id from the incoming message if it exists\n            existing_session_id = message.session_id\n        else:\n            message = Message(text=text)\n            existing_session_id = None\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        # Preserve session_id from incoming message, or use component/graph session_id\n        message.session_id = (\n            self.session_id or existing_session_id or (self.graph.session_id if hasattr(self, \"graph\") else None) or \"\"\n        )\n        message.context_id = self.context_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if message.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "height": 234,
        "id": "ChatOutput-lrulI",
        "measured": {
          "height": 234,
          "width": 320
        },
        "position": {
          "x": 1460.070372772908,
          "y": 872.7273956769025
        },
        "positionAbsolute": {
          "x": 1444.936881624563,
          "y": 872.7273956769025
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "id": "OpenRouterComponent-j1wSC",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "OpenRouter provides unified access to multiple AI models from different providers through a single API.",
            "display_name": "OpenRouter",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "api_key",
              "model_name",
              "temperature",
              "max_tokens",
              "site_url",
              "app_name"
            ],
            "frozen": false,
            "icon": "OpenRouter",
            "last_updated": "2026-01-20T03:27:01.664Z",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "83c3c312a7a2",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "httpx",
                    "version": "0.28.1"
                  },
                  {
                    "name": "langchain_openai",
                    "version": "0.3.35"
                  },
                  {
                    "name": "pydantic",
                    "version": "2.11.10"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 4
              },
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ],
              "module": "lfx.components.openrouter.openrouter.OpenRouterComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "loop_types": null,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "loop_types": null,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_frontend_node_flow_id": {
                "value": "8bcdb301-e0ad-48b2-8c25-fe5925b73940"
              },
              "_frontend_node_folder_id": {
                "value": "5abc5f52-5ec3-4b44-b9b2-c1fb69504bf7"
              },
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "API Key",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": true,
                "name": "api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "app_name": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "App Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "app_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import httpx\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom lfx.base.models.model import LCModelComponent\nfrom lfx.field_typing import LanguageModel\nfrom lfx.field_typing.range_spec import RangeSpec\nfrom lfx.inputs.inputs import DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\n\n\nclass OpenRouterComponent(LCModelComponent):\n    \"\"\"OpenRouter API component for language models.\"\"\"\n\n    display_name = \"OpenRouter\"\n    description = (\n        \"OpenRouter provides unified access to multiple AI models from different providers through a single API.\"\n    )\n    icon = \"OpenRouter\"\n\n    inputs = [\n        *LCModelComponent.get_base_inputs(),\n        SecretStrInput(name=\"api_key\", display_name=\"API Key\", required=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            options=[],\n            value=\"\",\n            refresh_button=True,\n            real_time_refresh=True,\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.7,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            advanced=True,\n        ),\n        IntInput(name=\"max_tokens\", display_name=\"Max Tokens\", advanced=True),\n        StrInput(name=\"site_url\", display_name=\"Site URL\", advanced=True),\n        StrInput(name=\"app_name\", display_name=\"App Name\", advanced=True),\n    ]\n\n    def fetch_models(self) -> list[dict]:\n        \"\"\"Fetch available models from OpenRouter.\"\"\"\n        try:\n            response = httpx.get(\"https://openrouter.ai/api/v1/models\", timeout=10.0)\n            response.raise_for_status()\n            models = response.json().get(\"data\", [])\n            return sorted(\n                [\n                    {\n                        \"id\": m[\"id\"],\n                        \"name\": m.get(\"name\", m[\"id\"]),\n                        \"context\": m.get(\"context_length\", 0),\n                    }\n                    for m in models\n                    if m.get(\"id\")\n                ],\n                key=lambda x: x[\"name\"],\n            )\n        except (httpx.RequestError, httpx.HTTPStatusError) as e:\n            self.log(f\"Error fetching models: {e}\")\n            return []\n\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None) -> dict:  # noqa: ARG002\n        \"\"\"Update model options.\"\"\"\n        models = self.fetch_models()\n        if models:\n            build_config[\"model_name\"][\"options\"] = [m[\"id\"] for m in models]\n            build_config[\"model_name\"][\"tooltips\"] = {m[\"id\"]: f\"{m['name']} ({m['context']:,} tokens)\" for m in models}\n        else:\n            build_config[\"model_name\"][\"options\"] = [\"Failed to load models\"]\n            build_config[\"model_name\"][\"value\"] = \"Failed to load models\"\n        return build_config\n\n    def build_model(self) -> LanguageModel:\n        \"\"\"Build the OpenRouter model.\"\"\"\n        if not self.api_key:\n            msg = \"API key is required\"\n            raise ValueError(msg)\n        if not self.model_name or self.model_name == \"Loading...\":\n            msg = \"Please select a model\"\n            raise ValueError(msg)\n\n        kwargs = {\n            \"model\": self.model_name,\n            \"openai_api_key\": SecretStr(self.api_key).get_secret_value(),\n            \"openai_api_base\": \"https://openrouter.ai/api/v1\",\n            \"temperature\": self.temperature if self.temperature is not None else 0.7,\n        }\n\n        if self.max_tokens:\n            kwargs[\"max_tokens\"] = int(self.max_tokens)\n\n        headers = {}\n        if self.site_url:\n            headers[\"HTTP-Referer\"] = self.site_url\n        if self.app_name:\n            headers[\"X-Title\"] = self.app_name\n        if headers:\n            kwargs[\"default_headers\"] = headers\n\n        return ChatOpenAI(**kwargs)\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "is_refresh": false,
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": ""
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "model_name",
                "options": [
                  "ai21/jamba-large-1.7",
                  "ai21/jamba-mini-1.7",
                  "aion-labs/aion-1.0",
                  "aion-labs/aion-1.0-mini",
                  "aion-labs/aion-rp-llama-3.1-8b",
                  "alfredpros/codellama-7b-instruct-solidity",
                  "allenai/molmo-2-8b:free",
                  "allenai/olmo-2-0325-32b-instruct",
                  "allenai/olmo-3-32b-think",
                  "allenai/olmo-3-7b-instruct",
                  "allenai/olmo-3-7b-think",
                  "allenai/olmo-3.1-32b-instruct",
                  "allenai/olmo-3.1-32b-think",
                  "amazon/nova-2-lite-v1",
                  "amazon/nova-lite-v1",
                  "amazon/nova-micro-v1",
                  "amazon/nova-premier-v1",
                  "amazon/nova-pro-v1",
                  "anthropic/claude-3-haiku",
                  "anthropic/claude-3.5-haiku",
                  "anthropic/claude-3.5-sonnet",
                  "anthropic/claude-3.7-sonnet",
                  "anthropic/claude-3.7-sonnet:thinking",
                  "anthropic/claude-haiku-4.5",
                  "anthropic/claude-opus-4",
                  "anthropic/claude-opus-4.1",
                  "anthropic/claude-opus-4.5",
                  "anthropic/claude-sonnet-4",
                  "anthropic/claude-sonnet-4.5",
                  "arcee-ai/coder-large",
                  "arcee-ai/maestro-reasoning",
                  "arcee-ai/spotlight",
                  "arcee-ai/trinity-mini",
                  "arcee-ai/trinity-mini:free",
                  "arcee-ai/virtuoso-large",
                  "openrouter/auto",
                  "baidu/ernie-4.5-21b-a3b",
                  "baidu/ernie-4.5-21b-a3b-thinking",
                  "baidu/ernie-4.5-300b-a47b",
                  "baidu/ernie-4.5-vl-28b-a3b",
                  "baidu/ernie-4.5-vl-424b-a47b",
                  "openrouter/bodybuilder",
                  "bytedance-seed/seed-1.6",
                  "bytedance-seed/seed-1.6-flash",
                  "bytedance/ui-tars-1.5-7b",
                  "deepcogito/cogito-v2-preview-llama-109b-moe",
                  "cohere/command-a",
                  "cohere/command-r-08-2024",
                  "cohere/command-r-plus-08-2024",
                  "cohere/command-r7b-12-2024",
                  "deepcogito/cogito-v2-preview-llama-405b",
                  "deepcogito/cogito-v2-preview-llama-70b",
                  "deepcogito/cogito-v2.1-671b",
                  "deepseek/deepseek-chat",
                  "deepseek/deepseek-chat-v3-0324",
                  "deepseek/deepseek-chat-v3.1",
                  "deepseek/deepseek-v3.1-terminus",
                  "deepseek/deepseek-v3.1-terminus:exacto",
                  "deepseek/deepseek-v3.2",
                  "deepseek/deepseek-v3.2-exp",
                  "deepseek/deepseek-v3.2-speciale",
                  "deepseek/deepseek-r1",
                  "deepseek/deepseek-r1-0528",
                  "deepseek/deepseek-r1-0528:free",
                  "deepseek/deepseek-r1-distill-llama-70b",
                  "deepseek/deepseek-r1-distill-qwen-32b",
                  "eleutherai/llemma_7b",
                  "essentialai/rnj-1-instruct",
                  "alpindale/goliath-120b",
                  "google/gemini-2.0-flash-001",
                  "google/gemini-2.0-flash-exp:free",
                  "google/gemini-2.0-flash-lite-001",
                  "google/gemini-2.5-flash",
                  "google/gemini-2.5-flash-image",
                  "google/gemini-2.5-flash-lite",
                  "google/gemini-2.5-flash-lite-preview-09-2025",
                  "google/gemini-2.5-flash-preview-09-2025",
                  "google/gemini-2.5-pro",
                  "google/gemini-2.5-pro-preview-05-06",
                  "google/gemini-2.5-pro-preview",
                  "google/gemini-3-flash-preview",
                  "google/gemini-3-pro-preview",
                  "google/gemma-2-27b-it",
                  "google/gemma-2-9b-it",
                  "google/gemma-3-12b-it",
                  "google/gemma-3-12b-it:free",
                  "google/gemma-3-27b-it",
                  "google/gemma-3-27b-it:free",
                  "google/gemma-3-4b-it",
                  "google/gemma-3-4b-it:free",
                  "google/gemma-3n-e2b-it:free",
                  "google/gemma-3n-e4b-it",
                  "google/gemma-3n-e4b-it:free",
                  "google/gemini-3-pro-image-preview",
                  "ibm-granite/granite-4.0-h-micro",
                  "inception/mercury",
                  "inception/mercury-coder",
                  "inflection/inflection-3-pi",
                  "inflection/inflection-3-productivity",
                  "kwaipilot/kat-coder-pro",
                  "liquid/lfm-2.2-6b",
                  "liquid/lfm2-8b-a1b",
                  "meta-llama/llama-guard-3-8b",
                  "anthracite-org/magnum-v4-72b",
                  "mancer/weaver",
                  "meituan/longcat-flash-chat",
                  "meta-llama/llama-3-70b-instruct",
                  "meta-llama/llama-3-8b-instruct",
                  "meta-llama/llama-3.1-405b",
                  "meta-llama/llama-3.1-405b-instruct",
                  "meta-llama/llama-3.1-405b-instruct:free",
                  "meta-llama/llama-3.1-70b-instruct",
                  "meta-llama/llama-3.1-8b-instruct",
                  "meta-llama/llama-3.2-11b-vision-instruct",
                  "meta-llama/llama-3.2-1b-instruct",
                  "meta-llama/llama-3.2-3b-instruct",
                  "meta-llama/llama-3.2-3b-instruct:free",
                  "meta-llama/llama-3.3-70b-instruct",
                  "meta-llama/llama-3.3-70b-instruct:free",
                  "meta-llama/llama-4-maverick",
                  "meta-llama/llama-4-scout",
                  "meta-llama/llama-guard-4-12b",
                  "meta-llama/llama-guard-2-8b",
                  "microsoft/phi-4",
                  "minimax/minimax-m1",
                  "minimax/minimax-m2",
                  "minimax/minimax-m2.1",
                  "minimax/minimax-01",
                  "mistralai/mistral-large",
                  "mistralai/mistral-large-2407",
                  "mistralai/mistral-large-2411",
                  "mistralai/mistral-tiny",
                  "mistralai/codestral-2508",
                  "mistralai/devstral-2512",
                  "mistralai/devstral-2512:free",
                  "mistralai/devstral-medium",
                  "mistralai/devstral-small",
                  "mistralai/ministral-14b-2512",
                  "mistralai/ministral-3b-2512",
                  "mistralai/ministral-8b-2512",
                  "mistralai/ministral-3b",
                  "mistralai/ministral-8b",
                  "mistralai/mistral-7b-instruct",
                  "mistralai/mistral-7b-instruct-v0.1",
                  "mistralai/mistral-7b-instruct-v0.2",
                  "mistralai/mistral-7b-instruct-v0.3",
                  "mistralai/mistral-large-2512",
                  "mistralai/mistral-medium-3",
                  "mistralai/mistral-medium-3.1",
                  "mistralai/mistral-nemo",
                  "mistralai/mistral-small-24b-instruct-2501",
                  "mistralai/mistral-small-3.1-24b-instruct",
                  "mistralai/mistral-small-3.1-24b-instruct:free",
                  "mistralai/mistral-small-3.2-24b-instruct",
                  "mistralai/mistral-small-creative",
                  "mistralai/mixtral-8x22b-instruct",
                  "mistralai/mixtral-8x7b-instruct",
                  "mistralai/pixtral-12b",
                  "mistralai/pixtral-large-2411",
                  "mistralai/mistral-saba",
                  "mistralai/voxtral-small-24b-2507",
                  "moonshotai/kimi-dev-72b",
                  "moonshotai/kimi-k2",
                  "moonshotai/kimi-k2:free",
                  "moonshotai/kimi-k2-0905",
                  "moonshotai/kimi-k2-0905:exacto",
                  "moonshotai/kimi-k2-thinking",
                  "morph/morph-v3-fast",
                  "morph/morph-v3-large",
                  "gryphe/mythomax-l2-13b",
                  "nvidia/llama-3.1-nemotron-70b-instruct",
                  "nvidia/llama-3.1-nemotron-ultra-253b-v1",
                  "nvidia/llama-3.3-nemotron-super-49b-v1.5",
                  "nvidia/nemotron-3-nano-30b-a3b",
                  "nvidia/nemotron-3-nano-30b-a3b:free",
                  "nvidia/nemotron-nano-12b-v2-vl",
                  "nvidia/nemotron-nano-12b-v2-vl:free",
                  "nvidia/nemotron-nano-9b-v2",
                  "nvidia/nemotron-nano-9b-v2:free",
                  "neversleep/llama-3.1-lumimaid-8b",
                  "nex-agi/deepseek-v3.1-nex-n1",
                  "neversleep/noromaid-20b",
                  "nousresearch/deephermes-3-mistral-24b-preview",
                  "nousresearch/hermes-3-llama-3.1-405b",
                  "nousresearch/hermes-3-llama-3.1-405b:free",
                  "nousresearch/hermes-3-llama-3.1-70b",
                  "nousresearch/hermes-4-405b",
                  "nousresearch/hermes-4-70b",
                  "nousresearch/hermes-2-pro-llama-3-8b",
                  "openai/chatgpt-4o-latest",
                  "openai/gpt-audio",
                  "openai/gpt-audio-mini",
                  "openai/gpt-3.5-turbo",
                  "openai/gpt-3.5-turbo-0613",
                  "openai/gpt-3.5-turbo-16k",
                  "openai/gpt-3.5-turbo-instruct",
                  "openai/gpt-4",
                  "openai/gpt-4-0314",
                  "openai/gpt-4-turbo",
                  "openai/gpt-4-1106-preview",
                  "openai/gpt-4-turbo-preview",
                  "openai/gpt-4.1",
                  "openai/gpt-4.1-mini",
                  "openai/gpt-4.1-nano",
                  "openai/gpt-4o",
                  "openai/gpt-4o-2024-05-13",
                  "openai/gpt-4o-2024-08-06",
                  "openai/gpt-4o-2024-11-20",
                  "openai/gpt-4o:extended",
                  "openai/gpt-4o-audio-preview",
                  "openai/gpt-4o-search-preview",
                  "openai/gpt-4o-mini",
                  "openai/gpt-4o-mini-2024-07-18",
                  "openai/gpt-4o-mini-search-preview",
                  "openai/gpt-5",
                  "openai/gpt-5-chat",
                  "openai/gpt-5-codex",
                  "openai/gpt-5-image",
                  "openai/gpt-5-image-mini",
                  "openai/gpt-5-mini",
                  "openai/gpt-5-nano",
                  "openai/gpt-5-pro",
                  "openai/gpt-5.1",
                  "openai/gpt-5.1-chat",
                  "openai/gpt-5.1-codex",
                  "openai/gpt-5.1-codex-max",
                  "openai/gpt-5.1-codex-mini",
                  "openai/gpt-5.2",
                  "openai/gpt-5.2-chat",
                  "openai/gpt-5.2-pro",
                  "openai/gpt-5.2-codex",
                  "openai/gpt-oss-120b",
                  "openai/gpt-oss-120b:exacto",
                  "openai/gpt-oss-120b:free",
                  "openai/gpt-oss-20b",
                  "openai/gpt-oss-20b:free",
                  "openai/gpt-oss-safeguard-20b",
                  "openai/o1",
                  "openai/o1-pro",
                  "openai/o3",
                  "openai/o3-deep-research",
                  "openai/o3-mini",
                  "openai/o3-mini-high",
                  "openai/o3-pro",
                  "openai/o4-mini",
                  "openai/o4-mini-deep-research",
                  "openai/o4-mini-high",
                  "opengvlab/internvl3-78b",
                  "perplexity/sonar",
                  "perplexity/sonar-deep-research",
                  "perplexity/sonar-pro",
                  "perplexity/sonar-pro-search",
                  "perplexity/sonar-reasoning-pro",
                  "prime-intellect/intellect-3",
                  "qwen/qwen-2.5-72b-instruct",
                  "qwen/qwen-2.5-coder-32b-instruct",
                  "qwen/qwq-32b",
                  "qwen/qwen-plus-2025-07-28",
                  "qwen/qwen-plus-2025-07-28:thinking",
                  "qwen/qwen-vl-max",
                  "qwen/qwen-vl-plus",
                  "qwen/qwen-max",
                  "qwen/qwen-plus",
                  "qwen/qwen-turbo",
                  "qwen/qwen-2.5-7b-instruct",
                  "qwen/qwen2.5-coder-7b-instruct",
                  "qwen/qwen2.5-vl-32b-instruct",
                  "qwen/qwen2.5-vl-72b-instruct",
                  "qwen/qwen-2.5-vl-7b-instruct",
                  "qwen/qwen-2.5-vl-7b-instruct:free",
                  "qwen/qwen3-14b",
                  "qwen/qwen3-235b-a22b",
                  "qwen/qwen3-235b-a22b-2507",
                  "qwen/qwen3-235b-a22b-thinking-2507",
                  "qwen/qwen3-30b-a3b",
                  "qwen/qwen3-30b-a3b-instruct-2507",
                  "qwen/qwen3-30b-a3b-thinking-2507",
                  "qwen/qwen3-32b",
                  "qwen/qwen3-4b:free",
                  "qwen/qwen3-8b",
                  "qwen/qwen3-coder-30b-a3b-instruct",
                  "qwen/qwen3-coder",
                  "qwen/qwen3-coder:exacto",
                  "qwen/qwen3-coder:free",
                  "qwen/qwen3-coder-flash",
                  "qwen/qwen3-coder-plus",
                  "qwen/qwen3-max",
                  "qwen/qwen3-next-80b-a3b-instruct",
                  "qwen/qwen3-next-80b-a3b-instruct:free",
                  "qwen/qwen3-next-80b-a3b-thinking",
                  "qwen/qwen3-vl-235b-a22b-instruct",
                  "qwen/qwen3-vl-235b-a22b-thinking",
                  "qwen/qwen3-vl-30b-a3b-instruct",
                  "qwen/qwen3-vl-30b-a3b-thinking",
                  "qwen/qwen3-vl-32b-instruct",
                  "qwen/qwen3-vl-8b-instruct",
                  "qwen/qwen3-vl-8b-thinking",
                  "undi95/remm-slerp-l2-13b",
                  "relace/relace-apply-3",
                  "relace/relace-search",
                  "sao10k/l3-lunaris-8b",
                  "sao10k/l3.1-70b-hanami-x1",
                  "sao10k/l3.1-euryale-70b",
                  "sao10k/l3.3-euryale-70b",
                  "sao10k/l3-euryale-70b",
                  "raifle/sorcererlm-8x22b",
                  "stepfun-ai/step3",
                  "switchpoint/router",
                  "tngtech/deepseek-r1t-chimera",
                  "tngtech/deepseek-r1t-chimera:free",
                  "tngtech/deepseek-r1t2-chimera",
                  "tngtech/deepseek-r1t2-chimera:free",
                  "tngtech/tng-r1t-chimera",
                  "tngtech/tng-r1t-chimera:free",
                  "tencent/hunyuan-a13b-instruct",
                  "thedrummer/cydonia-24b-v4.1",
                  "thedrummer/rocinante-12b",
                  "thedrummer/skyfall-36b-v2",
                  "thedrummer/unslopnemo-12b",
                  "alibaba/tongyi-deepresearch-30b-a3b",
                  "cognitivecomputations/dolphin-mistral-24b-venice-edition:free",
                  "microsoft/wizardlm-2-8x22b",
                  "xiaomi/mimo-v2-flash",
                  "xiaomi/mimo-v2-flash:free",
                  "z-ai/glm-4-32b",
                  "z-ai/glm-4.5",
                  "z-ai/glm-4.5-air",
                  "z-ai/glm-4.5-air:free",
                  "z-ai/glm-4.5v",
                  "z-ai/glm-4.6",
                  "z-ai/glm-4.6:exacto",
                  "z-ai/glm-4.6v",
                  "z-ai/glm-4.7",
                  "z-ai/glm-4.7-flash",
                  "x-ai/grok-3",
                  "x-ai/grok-3-beta",
                  "x-ai/grok-3-mini",
                  "x-ai/grok-3-mini-beta",
                  "x-ai/grok-4",
                  "x-ai/grok-4-fast",
                  "x-ai/grok-4.1-fast",
                  "x-ai/grok-code-fast-1"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "tooltips": {
                  "ai21/jamba-large-1.7": "AI21: Jamba Large 1.7 (256,000 tokens)",
                  "ai21/jamba-mini-1.7": "AI21: Jamba Mini 1.7 (256,000 tokens)",
                  "aion-labs/aion-1.0": "AionLabs: Aion-1.0 (131,072 tokens)",
                  "aion-labs/aion-1.0-mini": "AionLabs: Aion-1.0-Mini (131,072 tokens)",
                  "aion-labs/aion-rp-llama-3.1-8b": "AionLabs: Aion-RP 1.0 (8B) (32,768 tokens)",
                  "alfredpros/codellama-7b-instruct-solidity": "AlfredPros: CodeLLaMa 7B Instruct Solidity (4,096 tokens)",
                  "alibaba/tongyi-deepresearch-30b-a3b": "Tongyi DeepResearch 30B A3B (131,072 tokens)",
                  "allenai/molmo-2-8b:free": "AllenAI: Molmo2 8B (free) (36,864 tokens)",
                  "allenai/olmo-2-0325-32b-instruct": "AllenAI: Olmo 2 32B Instruct (128,000 tokens)",
                  "allenai/olmo-3-32b-think": "AllenAI: Olmo 3 32B Think (65,536 tokens)",
                  "allenai/olmo-3-7b-instruct": "AllenAI: Olmo 3 7B Instruct (65,536 tokens)",
                  "allenai/olmo-3-7b-think": "AllenAI: Olmo 3 7B Think (65,536 tokens)",
                  "allenai/olmo-3.1-32b-instruct": "AllenAI: Olmo 3.1 32B Instruct (65,536 tokens)",
                  "allenai/olmo-3.1-32b-think": "AllenAI: Olmo 3.1 32B Think (65,536 tokens)",
                  "alpindale/goliath-120b": "Goliath 120B (6,144 tokens)",
                  "amazon/nova-2-lite-v1": "Amazon: Nova 2 Lite (1,000,000 tokens)",
                  "amazon/nova-lite-v1": "Amazon: Nova Lite 1.0 (300,000 tokens)",
                  "amazon/nova-micro-v1": "Amazon: Nova Micro 1.0 (128,000 tokens)",
                  "amazon/nova-premier-v1": "Amazon: Nova Premier 1.0 (1,000,000 tokens)",
                  "amazon/nova-pro-v1": "Amazon: Nova Pro 1.0 (300,000 tokens)",
                  "anthracite-org/magnum-v4-72b": "Magnum v4 72B (16,384 tokens)",
                  "anthropic/claude-3-haiku": "Anthropic: Claude 3 Haiku (200,000 tokens)",
                  "anthropic/claude-3.5-haiku": "Anthropic: Claude 3.5 Haiku (200,000 tokens)",
                  "anthropic/claude-3.5-sonnet": "Anthropic: Claude 3.5 Sonnet (200,000 tokens)",
                  "anthropic/claude-3.7-sonnet": "Anthropic: Claude 3.7 Sonnet (200,000 tokens)",
                  "anthropic/claude-3.7-sonnet:thinking": "Anthropic: Claude 3.7 Sonnet (thinking) (200,000 tokens)",
                  "anthropic/claude-haiku-4.5": "Anthropic: Claude Haiku 4.5 (200,000 tokens)",
                  "anthropic/claude-opus-4": "Anthropic: Claude Opus 4 (200,000 tokens)",
                  "anthropic/claude-opus-4.1": "Anthropic: Claude Opus 4.1 (200,000 tokens)",
                  "anthropic/claude-opus-4.5": "Anthropic: Claude Opus 4.5 (200,000 tokens)",
                  "anthropic/claude-sonnet-4": "Anthropic: Claude Sonnet 4 (1,000,000 tokens)",
                  "anthropic/claude-sonnet-4.5": "Anthropic: Claude Sonnet 4.5 (1,000,000 tokens)",
                  "arcee-ai/coder-large": "Arcee AI: Coder Large (32,768 tokens)",
                  "arcee-ai/maestro-reasoning": "Arcee AI: Maestro Reasoning (131,072 tokens)",
                  "arcee-ai/spotlight": "Arcee AI: Spotlight (131,072 tokens)",
                  "arcee-ai/trinity-mini": "Arcee AI: Trinity Mini (131,072 tokens)",
                  "arcee-ai/trinity-mini:free": "Arcee AI: Trinity Mini (free) (131,072 tokens)",
                  "arcee-ai/virtuoso-large": "Arcee AI: Virtuoso Large (131,072 tokens)",
                  "baidu/ernie-4.5-21b-a3b": "Baidu: ERNIE 4.5 21B A3B (120,000 tokens)",
                  "baidu/ernie-4.5-21b-a3b-thinking": "Baidu: ERNIE 4.5 21B A3B Thinking (131,072 tokens)",
                  "baidu/ernie-4.5-300b-a47b": "Baidu: ERNIE 4.5 300B A47B  (123,000 tokens)",
                  "baidu/ernie-4.5-vl-28b-a3b": "Baidu: ERNIE 4.5 VL 28B A3B (30,000 tokens)",
                  "baidu/ernie-4.5-vl-424b-a47b": "Baidu: ERNIE 4.5 VL 424B A47B  (123,000 tokens)",
                  "bytedance-seed/seed-1.6": "ByteDance Seed: Seed 1.6 (262,144 tokens)",
                  "bytedance-seed/seed-1.6-flash": "ByteDance Seed: Seed 1.6 Flash (262,144 tokens)",
                  "bytedance/ui-tars-1.5-7b": "ByteDance: UI-TARS 7B  (128,000 tokens)",
                  "cognitivecomputations/dolphin-mistral-24b-venice-edition:free": "Venice: Uncensored (free) (32,768 tokens)",
                  "cohere/command-a": "Cohere: Command A (256,000 tokens)",
                  "cohere/command-r-08-2024": "Cohere: Command R (08-2024) (128,000 tokens)",
                  "cohere/command-r-plus-08-2024": "Cohere: Command R+ (08-2024) (128,000 tokens)",
                  "cohere/command-r7b-12-2024": "Cohere: Command R7B (12-2024) (128,000 tokens)",
                  "deepcogito/cogito-v2-preview-llama-109b-moe": "Cogito V2 Preview Llama 109B (32,767 tokens)",
                  "deepcogito/cogito-v2-preview-llama-405b": "Deep Cogito: Cogito V2 Preview Llama 405B (32,768 tokens)",
                  "deepcogito/cogito-v2-preview-llama-70b": "Deep Cogito: Cogito V2 Preview Llama 70B (32,768 tokens)",
                  "deepcogito/cogito-v2.1-671b": "Deep Cogito: Cogito v2.1 671B (128,000 tokens)",
                  "deepseek/deepseek-chat": "DeepSeek: DeepSeek V3 (163,840 tokens)",
                  "deepseek/deepseek-chat-v3-0324": "DeepSeek: DeepSeek V3 0324 (163,840 tokens)",
                  "deepseek/deepseek-chat-v3.1": "DeepSeek: DeepSeek V3.1 (32,768 tokens)",
                  "deepseek/deepseek-r1": "DeepSeek: R1 (64,000 tokens)",
                  "deepseek/deepseek-r1-0528": "DeepSeek: R1 0528 (163,840 tokens)",
                  "deepseek/deepseek-r1-0528:free": "DeepSeek: R1 0528 (free) (163,840 tokens)",
                  "deepseek/deepseek-r1-distill-llama-70b": "DeepSeek: R1 Distill Llama 70B (131,072 tokens)",
                  "deepseek/deepseek-r1-distill-qwen-32b": "DeepSeek: R1 Distill Qwen 32B (32,768 tokens)",
                  "deepseek/deepseek-v3.1-terminus": "DeepSeek: DeepSeek V3.1 Terminus (163,840 tokens)",
                  "deepseek/deepseek-v3.1-terminus:exacto": "DeepSeek: DeepSeek V3.1 Terminus (exacto) (163,840 tokens)",
                  "deepseek/deepseek-v3.2": "DeepSeek: DeepSeek V3.2 (163,840 tokens)",
                  "deepseek/deepseek-v3.2-exp": "DeepSeek: DeepSeek V3.2 Exp (163,840 tokens)",
                  "deepseek/deepseek-v3.2-speciale": "DeepSeek: DeepSeek V3.2 Speciale (163,840 tokens)",
                  "eleutherai/llemma_7b": "EleutherAI: Llemma 7b (4,096 tokens)",
                  "essentialai/rnj-1-instruct": "EssentialAI: Rnj 1 Instruct (32,768 tokens)",
                  "google/gemini-2.0-flash-001": "Google: Gemini 2.0 Flash (1,048,576 tokens)",
                  "google/gemini-2.0-flash-exp:free": "Google: Gemini 2.0 Flash Experimental (free) (1,048,576 tokens)",
                  "google/gemini-2.0-flash-lite-001": "Google: Gemini 2.0 Flash Lite (1,048,576 tokens)",
                  "google/gemini-2.5-flash": "Google: Gemini 2.5 Flash (1,048,576 tokens)",
                  "google/gemini-2.5-flash-image": "Google: Gemini 2.5 Flash Image (Nano Banana) (32,768 tokens)",
                  "google/gemini-2.5-flash-lite": "Google: Gemini 2.5 Flash Lite (1,048,576 tokens)",
                  "google/gemini-2.5-flash-lite-preview-09-2025": "Google: Gemini 2.5 Flash Lite Preview 09-2025 (1,048,576 tokens)",
                  "google/gemini-2.5-flash-preview-09-2025": "Google: Gemini 2.5 Flash Preview 09-2025 (1,048,576 tokens)",
                  "google/gemini-2.5-pro": "Google: Gemini 2.5 Pro (1,048,576 tokens)",
                  "google/gemini-2.5-pro-preview": "Google: Gemini 2.5 Pro Preview 06-05 (1,048,576 tokens)",
                  "google/gemini-2.5-pro-preview-05-06": "Google: Gemini 2.5 Pro Preview 05-06 (1,048,576 tokens)",
                  "google/gemini-3-flash-preview": "Google: Gemini 3 Flash Preview (1,048,576 tokens)",
                  "google/gemini-3-pro-image-preview": "Google: Nano Banana Pro (Gemini 3 Pro Image Preview) (65,536 tokens)",
                  "google/gemini-3-pro-preview": "Google: Gemini 3 Pro Preview (1,048,576 tokens)",
                  "google/gemma-2-27b-it": "Google: Gemma 2 27B (8,192 tokens)",
                  "google/gemma-2-9b-it": "Google: Gemma 2 9B (8,192 tokens)",
                  "google/gemma-3-12b-it": "Google: Gemma 3 12B (131,072 tokens)",
                  "google/gemma-3-12b-it:free": "Google: Gemma 3 12B (free) (32,768 tokens)",
                  "google/gemma-3-27b-it": "Google: Gemma 3 27B (96,000 tokens)",
                  "google/gemma-3-27b-it:free": "Google: Gemma 3 27B (free) (131,072 tokens)",
                  "google/gemma-3-4b-it": "Google: Gemma 3 4B (96,000 tokens)",
                  "google/gemma-3-4b-it:free": "Google: Gemma 3 4B (free) (32,768 tokens)",
                  "google/gemma-3n-e2b-it:free": "Google: Gemma 3n 2B (free) (8,192 tokens)",
                  "google/gemma-3n-e4b-it": "Google: Gemma 3n 4B (32,768 tokens)",
                  "google/gemma-3n-e4b-it:free": "Google: Gemma 3n 4B (free) (8,192 tokens)",
                  "gryphe/mythomax-l2-13b": "MythoMax 13B (4,096 tokens)",
                  "ibm-granite/granite-4.0-h-micro": "IBM: Granite 4.0 Micro (131,000 tokens)",
                  "inception/mercury": "Inception: Mercury (128,000 tokens)",
                  "inception/mercury-coder": "Inception: Mercury Coder (128,000 tokens)",
                  "inflection/inflection-3-pi": "Inflection: Inflection 3 Pi (8,000 tokens)",
                  "inflection/inflection-3-productivity": "Inflection: Inflection 3 Productivity (8,000 tokens)",
                  "kwaipilot/kat-coder-pro": "Kwaipilot: KAT-Coder-Pro V1 (256,000 tokens)",
                  "liquid/lfm-2.2-6b": "LiquidAI/LFM2-2.6B (32,768 tokens)",
                  "liquid/lfm2-8b-a1b": "LiquidAI/LFM2-8B-A1B (32,768 tokens)",
                  "mancer/weaver": "Mancer: Weaver (alpha) (8,000 tokens)",
                  "meituan/longcat-flash-chat": "Meituan: LongCat Flash Chat (131,072 tokens)",
                  "meta-llama/llama-3-70b-instruct": "Meta: Llama 3 70B Instruct (8,192 tokens)",
                  "meta-llama/llama-3-8b-instruct": "Meta: Llama 3 8B Instruct (8,192 tokens)",
                  "meta-llama/llama-3.1-405b": "Meta: Llama 3.1 405B (base) (32,768 tokens)",
                  "meta-llama/llama-3.1-405b-instruct": "Meta: Llama 3.1 405B Instruct (10,000 tokens)",
                  "meta-llama/llama-3.1-405b-instruct:free": "Meta: Llama 3.1 405B Instruct (free) (131,072 tokens)",
                  "meta-llama/llama-3.1-70b-instruct": "Meta: Llama 3.1 70B Instruct (131,072 tokens)",
                  "meta-llama/llama-3.1-8b-instruct": "Meta: Llama 3.1 8B Instruct (16,384 tokens)",
                  "meta-llama/llama-3.2-11b-vision-instruct": "Meta: Llama 3.2 11B Vision Instruct (131,072 tokens)",
                  "meta-llama/llama-3.2-1b-instruct": "Meta: Llama 3.2 1B Instruct (60,000 tokens)",
                  "meta-llama/llama-3.2-3b-instruct": "Meta: Llama 3.2 3B Instruct (131,072 tokens)",
                  "meta-llama/llama-3.2-3b-instruct:free": "Meta: Llama 3.2 3B Instruct (free) (131,072 tokens)",
                  "meta-llama/llama-3.3-70b-instruct": "Meta: Llama 3.3 70B Instruct (131,072 tokens)",
                  "meta-llama/llama-3.3-70b-instruct:free": "Meta: Llama 3.3 70B Instruct (free) (131,072 tokens)",
                  "meta-llama/llama-4-maverick": "Meta: Llama 4 Maverick (1,048,576 tokens)",
                  "meta-llama/llama-4-scout": "Meta: Llama 4 Scout (327,680 tokens)",
                  "meta-llama/llama-guard-2-8b": "Meta: LlamaGuard 2 8B (8,192 tokens)",
                  "meta-llama/llama-guard-3-8b": "Llama Guard 3 8B (131,072 tokens)",
                  "meta-llama/llama-guard-4-12b": "Meta: Llama Guard 4 12B (163,840 tokens)",
                  "microsoft/phi-4": "Microsoft: Phi 4 (16,384 tokens)",
                  "microsoft/wizardlm-2-8x22b": "WizardLM-2 8x22B (65,536 tokens)",
                  "minimax/minimax-01": "MiniMax: MiniMax-01 (1,000,192 tokens)",
                  "minimax/minimax-m1": "MiniMax: MiniMax M1 (1,000,000 tokens)",
                  "minimax/minimax-m2": "MiniMax: MiniMax M2 (196,608 tokens)",
                  "minimax/minimax-m2.1": "MiniMax: MiniMax M2.1 (196,608 tokens)",
                  "mistralai/codestral-2508": "Mistral: Codestral 2508 (256,000 tokens)",
                  "mistralai/devstral-2512": "Mistral: Devstral 2 2512 (262,144 tokens)",
                  "mistralai/devstral-2512:free": "Mistral: Devstral 2 2512 (free) (262,144 tokens)",
                  "mistralai/devstral-medium": "Mistral: Devstral Medium (131,072 tokens)",
                  "mistralai/devstral-small": "Mistral: Devstral Small 1.1 (131,072 tokens)",
                  "mistralai/ministral-14b-2512": "Mistral: Ministral 3 14B 2512 (262,144 tokens)",
                  "mistralai/ministral-3b": "Mistral: Ministral 3B (131,072 tokens)",
                  "mistralai/ministral-3b-2512": "Mistral: Ministral 3 3B 2512 (131,072 tokens)",
                  "mistralai/ministral-8b": "Mistral: Ministral 8B (131,072 tokens)",
                  "mistralai/ministral-8b-2512": "Mistral: Ministral 3 8B 2512 (262,144 tokens)",
                  "mistralai/mistral-7b-instruct": "Mistral: Mistral 7B Instruct (32,768 tokens)",
                  "mistralai/mistral-7b-instruct-v0.1": "Mistral: Mistral 7B Instruct v0.1 (2,824 tokens)",
                  "mistralai/mistral-7b-instruct-v0.2": "Mistral: Mistral 7B Instruct v0.2 (32,768 tokens)",
                  "mistralai/mistral-7b-instruct-v0.3": "Mistral: Mistral 7B Instruct v0.3 (32,768 tokens)",
                  "mistralai/mistral-large": "Mistral Large (128,000 tokens)",
                  "mistralai/mistral-large-2407": "Mistral Large 2407 (131,072 tokens)",
                  "mistralai/mistral-large-2411": "Mistral Large 2411 (131,072 tokens)",
                  "mistralai/mistral-large-2512": "Mistral: Mistral Large 3 2512 (262,144 tokens)",
                  "mistralai/mistral-medium-3": "Mistral: Mistral Medium 3 (131,072 tokens)",
                  "mistralai/mistral-medium-3.1": "Mistral: Mistral Medium 3.1 (131,072 tokens)",
                  "mistralai/mistral-nemo": "Mistral: Mistral Nemo (131,072 tokens)",
                  "mistralai/mistral-saba": "Mistral: Saba (32,768 tokens)",
                  "mistralai/mistral-small-24b-instruct-2501": "Mistral: Mistral Small 3 (32,768 tokens)",
                  "mistralai/mistral-small-3.1-24b-instruct": "Mistral: Mistral Small 3.1 24B (131,072 tokens)",
                  "mistralai/mistral-small-3.1-24b-instruct:free": "Mistral: Mistral Small 3.1 24B (free) (128,000 tokens)",
                  "mistralai/mistral-small-3.2-24b-instruct": "Mistral: Mistral Small 3.2 24B (131,072 tokens)",
                  "mistralai/mistral-small-creative": "Mistral: Mistral Small Creative (32,768 tokens)",
                  "mistralai/mistral-tiny": "Mistral Tiny (32,768 tokens)",
                  "mistralai/mixtral-8x22b-instruct": "Mistral: Mixtral 8x22B Instruct (65,536 tokens)",
                  "mistralai/mixtral-8x7b-instruct": "Mistral: Mixtral 8x7B Instruct (32,768 tokens)",
                  "mistralai/pixtral-12b": "Mistral: Pixtral 12B (32,768 tokens)",
                  "mistralai/pixtral-large-2411": "Mistral: Pixtral Large 2411 (131,072 tokens)",
                  "mistralai/voxtral-small-24b-2507": "Mistral: Voxtral Small 24B 2507 (32,000 tokens)",
                  "moonshotai/kimi-dev-72b": "MoonshotAI: Kimi Dev 72B (131,072 tokens)",
                  "moonshotai/kimi-k2": "MoonshotAI: Kimi K2 0711 (131,072 tokens)",
                  "moonshotai/kimi-k2-0905": "MoonshotAI: Kimi K2 0905 (262,144 tokens)",
                  "moonshotai/kimi-k2-0905:exacto": "MoonshotAI: Kimi K2 0905 (exacto) (262,144 tokens)",
                  "moonshotai/kimi-k2-thinking": "MoonshotAI: Kimi K2 Thinking (262,144 tokens)",
                  "moonshotai/kimi-k2:free": "MoonshotAI: Kimi K2 0711 (free) (32,768 tokens)",
                  "morph/morph-v3-fast": "Morph: Morph V3 Fast (81,920 tokens)",
                  "morph/morph-v3-large": "Morph: Morph V3 Large (262,144 tokens)",
                  "neversleep/llama-3.1-lumimaid-8b": "NeverSleep: Lumimaid v0.2 8B (32,768 tokens)",
                  "neversleep/noromaid-20b": "Noromaid 20B (4,096 tokens)",
                  "nex-agi/deepseek-v3.1-nex-n1": "Nex AGI: DeepSeek V3.1 Nex N1 (131,072 tokens)",
                  "nousresearch/deephermes-3-mistral-24b-preview": "Nous: DeepHermes 3 Mistral 24B Preview (32,768 tokens)",
                  "nousresearch/hermes-2-pro-llama-3-8b": "NousResearch: Hermes 2 Pro - Llama-3 8B (8,192 tokens)",
                  "nousresearch/hermes-3-llama-3.1-405b": "Nous: Hermes 3 405B Instruct (131,072 tokens)",
                  "nousresearch/hermes-3-llama-3.1-405b:free": "Nous: Hermes 3 405B Instruct (free) (131,072 tokens)",
                  "nousresearch/hermes-3-llama-3.1-70b": "Nous: Hermes 3 70B Instruct (65,536 tokens)",
                  "nousresearch/hermes-4-405b": "Nous: Hermes 4 405B (131,072 tokens)",
                  "nousresearch/hermes-4-70b": "Nous: Hermes 4 70B (131,072 tokens)",
                  "nvidia/llama-3.1-nemotron-70b-instruct": "NVIDIA: Llama 3.1 Nemotron 70B Instruct (131,072 tokens)",
                  "nvidia/llama-3.1-nemotron-ultra-253b-v1": "NVIDIA: Llama 3.1 Nemotron Ultra 253B v1 (131,072 tokens)",
                  "nvidia/llama-3.3-nemotron-super-49b-v1.5": "NVIDIA: Llama 3.3 Nemotron Super 49B V1.5 (131,072 tokens)",
                  "nvidia/nemotron-3-nano-30b-a3b": "NVIDIA: Nemotron 3 Nano 30B A3B (262,144 tokens)",
                  "nvidia/nemotron-3-nano-30b-a3b:free": "NVIDIA: Nemotron 3 Nano 30B A3B (free) (256,000 tokens)",
                  "nvidia/nemotron-nano-12b-v2-vl": "NVIDIA: Nemotron Nano 12B 2 VL (131,072 tokens)",
                  "nvidia/nemotron-nano-12b-v2-vl:free": "NVIDIA: Nemotron Nano 12B 2 VL (free) (128,000 tokens)",
                  "nvidia/nemotron-nano-9b-v2": "NVIDIA: Nemotron Nano 9B V2 (131,072 tokens)",
                  "nvidia/nemotron-nano-9b-v2:free": "NVIDIA: Nemotron Nano 9B V2 (free) (128,000 tokens)",
                  "openai/chatgpt-4o-latest": "OpenAI: ChatGPT-4o (128,000 tokens)",
                  "openai/gpt-3.5-turbo": "OpenAI: GPT-3.5 Turbo (16,385 tokens)",
                  "openai/gpt-3.5-turbo-0613": "OpenAI: GPT-3.5 Turbo (older v0613) (4,095 tokens)",
                  "openai/gpt-3.5-turbo-16k": "OpenAI: GPT-3.5 Turbo 16k (16,385 tokens)",
                  "openai/gpt-3.5-turbo-instruct": "OpenAI: GPT-3.5 Turbo Instruct (4,095 tokens)",
                  "openai/gpt-4": "OpenAI: GPT-4 (8,191 tokens)",
                  "openai/gpt-4-0314": "OpenAI: GPT-4 (older v0314) (8,191 tokens)",
                  "openai/gpt-4-1106-preview": "OpenAI: GPT-4 Turbo (older v1106) (128,000 tokens)",
                  "openai/gpt-4-turbo": "OpenAI: GPT-4 Turbo (128,000 tokens)",
                  "openai/gpt-4-turbo-preview": "OpenAI: GPT-4 Turbo Preview (128,000 tokens)",
                  "openai/gpt-4.1": "OpenAI: GPT-4.1 (1,047,576 tokens)",
                  "openai/gpt-4.1-mini": "OpenAI: GPT-4.1 Mini (1,047,576 tokens)",
                  "openai/gpt-4.1-nano": "OpenAI: GPT-4.1 Nano (1,047,576 tokens)",
                  "openai/gpt-4o": "OpenAI: GPT-4o (128,000 tokens)",
                  "openai/gpt-4o-2024-05-13": "OpenAI: GPT-4o (2024-05-13) (128,000 tokens)",
                  "openai/gpt-4o-2024-08-06": "OpenAI: GPT-4o (2024-08-06) (128,000 tokens)",
                  "openai/gpt-4o-2024-11-20": "OpenAI: GPT-4o (2024-11-20) (128,000 tokens)",
                  "openai/gpt-4o-audio-preview": "OpenAI: GPT-4o Audio (128,000 tokens)",
                  "openai/gpt-4o-mini": "OpenAI: GPT-4o-mini (128,000 tokens)",
                  "openai/gpt-4o-mini-2024-07-18": "OpenAI: GPT-4o-mini (2024-07-18) (128,000 tokens)",
                  "openai/gpt-4o-mini-search-preview": "OpenAI: GPT-4o-mini Search Preview (128,000 tokens)",
                  "openai/gpt-4o-search-preview": "OpenAI: GPT-4o Search Preview (128,000 tokens)",
                  "openai/gpt-4o:extended": "OpenAI: GPT-4o (extended) (128,000 tokens)",
                  "openai/gpt-5": "OpenAI: GPT-5 (400,000 tokens)",
                  "openai/gpt-5-chat": "OpenAI: GPT-5 Chat (128,000 tokens)",
                  "openai/gpt-5-codex": "OpenAI: GPT-5 Codex (400,000 tokens)",
                  "openai/gpt-5-image": "OpenAI: GPT-5 Image (400,000 tokens)",
                  "openai/gpt-5-image-mini": "OpenAI: GPT-5 Image Mini (400,000 tokens)",
                  "openai/gpt-5-mini": "OpenAI: GPT-5 Mini (400,000 tokens)",
                  "openai/gpt-5-nano": "OpenAI: GPT-5 Nano (400,000 tokens)",
                  "openai/gpt-5-pro": "OpenAI: GPT-5 Pro (400,000 tokens)",
                  "openai/gpt-5.1": "OpenAI: GPT-5.1 (400,000 tokens)",
                  "openai/gpt-5.1-chat": "OpenAI: GPT-5.1 Chat (128,000 tokens)",
                  "openai/gpt-5.1-codex": "OpenAI: GPT-5.1-Codex (400,000 tokens)",
                  "openai/gpt-5.1-codex-max": "OpenAI: GPT-5.1-Codex-Max (400,000 tokens)",
                  "openai/gpt-5.1-codex-mini": "OpenAI: GPT-5.1-Codex-Mini (400,000 tokens)",
                  "openai/gpt-5.2": "OpenAI: GPT-5.2 (400,000 tokens)",
                  "openai/gpt-5.2-chat": "OpenAI: GPT-5.2 Chat (128,000 tokens)",
                  "openai/gpt-5.2-codex": "OpenAI: GPT-5.2-Codex (400,000 tokens)",
                  "openai/gpt-5.2-pro": "OpenAI: GPT-5.2 Pro (400,000 tokens)",
                  "openai/gpt-audio": "OpenAI: GPT Audio (128,000 tokens)",
                  "openai/gpt-audio-mini": "OpenAI: GPT Audio Mini (128,000 tokens)",
                  "openai/gpt-oss-120b": "OpenAI: gpt-oss-120b (131,072 tokens)",
                  "openai/gpt-oss-120b:exacto": "OpenAI: gpt-oss-120b (exacto) (131,072 tokens)",
                  "openai/gpt-oss-120b:free": "OpenAI: gpt-oss-120b (free) (131,072 tokens)",
                  "openai/gpt-oss-20b": "OpenAI: gpt-oss-20b (131,072 tokens)",
                  "openai/gpt-oss-20b:free": "OpenAI: gpt-oss-20b (free) (131,072 tokens)",
                  "openai/gpt-oss-safeguard-20b": "OpenAI: gpt-oss-safeguard-20b (131,072 tokens)",
                  "openai/o1": "OpenAI: o1 (200,000 tokens)",
                  "openai/o1-pro": "OpenAI: o1-pro (200,000 tokens)",
                  "openai/o3": "OpenAI: o3 (200,000 tokens)",
                  "openai/o3-deep-research": "OpenAI: o3 Deep Research (200,000 tokens)",
                  "openai/o3-mini": "OpenAI: o3 Mini (200,000 tokens)",
                  "openai/o3-mini-high": "OpenAI: o3 Mini High (200,000 tokens)",
                  "openai/o3-pro": "OpenAI: o3 Pro (200,000 tokens)",
                  "openai/o4-mini": "OpenAI: o4 Mini (200,000 tokens)",
                  "openai/o4-mini-deep-research": "OpenAI: o4 Mini Deep Research (200,000 tokens)",
                  "openai/o4-mini-high": "OpenAI: o4 Mini High (200,000 tokens)",
                  "opengvlab/internvl3-78b": "OpenGVLab: InternVL3 78B (32,768 tokens)",
                  "openrouter/auto": "Auto Router (2,000,000 tokens)",
                  "openrouter/bodybuilder": "Body Builder (beta) (128,000 tokens)",
                  "perplexity/sonar": "Perplexity: Sonar (127,072 tokens)",
                  "perplexity/sonar-deep-research": "Perplexity: Sonar Deep Research (128,000 tokens)",
                  "perplexity/sonar-pro": "Perplexity: Sonar Pro (200,000 tokens)",
                  "perplexity/sonar-pro-search": "Perplexity: Sonar Pro Search (200,000 tokens)",
                  "perplexity/sonar-reasoning-pro": "Perplexity: Sonar Reasoning Pro (128,000 tokens)",
                  "prime-intellect/intellect-3": "Prime Intellect: INTELLECT-3 (131,072 tokens)",
                  "qwen/qwen-2.5-72b-instruct": "Qwen2.5 72B Instruct (32,768 tokens)",
                  "qwen/qwen-2.5-7b-instruct": "Qwen: Qwen2.5 7B Instruct (32,768 tokens)",
                  "qwen/qwen-2.5-coder-32b-instruct": "Qwen2.5 Coder 32B Instruct (32,768 tokens)",
                  "qwen/qwen-2.5-vl-7b-instruct": "Qwen: Qwen2.5-VL 7B Instruct (32,768 tokens)",
                  "qwen/qwen-2.5-vl-7b-instruct:free": "Qwen: Qwen2.5-VL 7B Instruct (free) (32,768 tokens)",
                  "qwen/qwen-max": "Qwen: Qwen-Max  (32,768 tokens)",
                  "qwen/qwen-plus": "Qwen: Qwen-Plus (131,072 tokens)",
                  "qwen/qwen-plus-2025-07-28": "Qwen: Qwen Plus 0728 (1,000,000 tokens)",
                  "qwen/qwen-plus-2025-07-28:thinking": "Qwen: Qwen Plus 0728 (thinking) (1,000,000 tokens)",
                  "qwen/qwen-turbo": "Qwen: Qwen-Turbo (1,000,000 tokens)",
                  "qwen/qwen-vl-max": "Qwen: Qwen VL Max (131,072 tokens)",
                  "qwen/qwen-vl-plus": "Qwen: Qwen VL Plus (7,500 tokens)",
                  "qwen/qwen2.5-coder-7b-instruct": "Qwen: Qwen2.5 Coder 7B Instruct (32,768 tokens)",
                  "qwen/qwen2.5-vl-32b-instruct": "Qwen: Qwen2.5 VL 32B Instruct (16,384 tokens)",
                  "qwen/qwen2.5-vl-72b-instruct": "Qwen: Qwen2.5 VL 72B Instruct (32,768 tokens)",
                  "qwen/qwen3-14b": "Qwen: Qwen3 14B (40,960 tokens)",
                  "qwen/qwen3-235b-a22b": "Qwen: Qwen3 235B A22B (40,960 tokens)",
                  "qwen/qwen3-235b-a22b-2507": "Qwen: Qwen3 235B A22B Instruct 2507 (262,144 tokens)",
                  "qwen/qwen3-235b-a22b-thinking-2507": "Qwen: Qwen3 235B A22B Thinking 2507 (262,144 tokens)",
                  "qwen/qwen3-30b-a3b": "Qwen: Qwen3 30B A3B (40,960 tokens)",
                  "qwen/qwen3-30b-a3b-instruct-2507": "Qwen: Qwen3 30B A3B Instruct 2507 (262,144 tokens)",
                  "qwen/qwen3-30b-a3b-thinking-2507": "Qwen: Qwen3 30B A3B Thinking 2507 (32,768 tokens)",
                  "qwen/qwen3-32b": "Qwen: Qwen3 32B (40,960 tokens)",
                  "qwen/qwen3-4b:free": "Qwen: Qwen3 4B (free) (40,960 tokens)",
                  "qwen/qwen3-8b": "Qwen: Qwen3 8B (32,000 tokens)",
                  "qwen/qwen3-coder": "Qwen: Qwen3 Coder 480B A35B (262,144 tokens)",
                  "qwen/qwen3-coder-30b-a3b-instruct": "Qwen: Qwen3 Coder 30B A3B Instruct (160,000 tokens)",
                  "qwen/qwen3-coder-flash": "Qwen: Qwen3 Coder Flash (128,000 tokens)",
                  "qwen/qwen3-coder-plus": "Qwen: Qwen3 Coder Plus (128,000 tokens)",
                  "qwen/qwen3-coder:exacto": "Qwen: Qwen3 Coder 480B A35B (exacto) (262,144 tokens)",
                  "qwen/qwen3-coder:free": "Qwen: Qwen3 Coder 480B A35B (free) (262,000 tokens)",
                  "qwen/qwen3-max": "Qwen: Qwen3 Max (256,000 tokens)",
                  "qwen/qwen3-next-80b-a3b-instruct": "Qwen: Qwen3 Next 80B A3B Instruct (262,144 tokens)",
                  "qwen/qwen3-next-80b-a3b-instruct:free": "Qwen: Qwen3 Next 80B A3B Instruct (free) (262,144 tokens)",
                  "qwen/qwen3-next-80b-a3b-thinking": "Qwen: Qwen3 Next 80B A3B Thinking (128,000 tokens)",
                  "qwen/qwen3-vl-235b-a22b-instruct": "Qwen: Qwen3 VL 235B A22B Instruct (262,144 tokens)",
                  "qwen/qwen3-vl-235b-a22b-thinking": "Qwen: Qwen3 VL 235B A22B Thinking (131,072 tokens)",
                  "qwen/qwen3-vl-30b-a3b-instruct": "Qwen: Qwen3 VL 30B A3B Instruct (262,144 tokens)",
                  "qwen/qwen3-vl-30b-a3b-thinking": "Qwen: Qwen3 VL 30B A3B Thinking (131,072 tokens)",
                  "qwen/qwen3-vl-32b-instruct": "Qwen: Qwen3 VL 32B Instruct (262,144 tokens)",
                  "qwen/qwen3-vl-8b-instruct": "Qwen: Qwen3 VL 8B Instruct (131,072 tokens)",
                  "qwen/qwen3-vl-8b-thinking": "Qwen: Qwen3 VL 8B Thinking (256,000 tokens)",
                  "qwen/qwq-32b": "Qwen: QwQ 32B (32,768 tokens)",
                  "raifle/sorcererlm-8x22b": "SorcererLM 8x22B (16,000 tokens)",
                  "relace/relace-apply-3": "Relace: Relace Apply 3 (256,000 tokens)",
                  "relace/relace-search": "Relace: Relace Search (256,000 tokens)",
                  "sao10k/l3-euryale-70b": "Sao10k: Llama 3 Euryale 70B v2.1 (8,192 tokens)",
                  "sao10k/l3-lunaris-8b": "Sao10K: Llama 3 8B Lunaris (8,192 tokens)",
                  "sao10k/l3.1-70b-hanami-x1": "Sao10K: Llama 3.1 70B Hanami x1 (16,000 tokens)",
                  "sao10k/l3.1-euryale-70b": "Sao10K: Llama 3.1 Euryale 70B v2.2 (32,768 tokens)",
                  "sao10k/l3.3-euryale-70b": "Sao10K: Llama 3.3 Euryale 70B (131,072 tokens)",
                  "stepfun-ai/step3": "StepFun: Step3 (65,536 tokens)",
                  "switchpoint/router": "Switchpoint Router (131,072 tokens)",
                  "tencent/hunyuan-a13b-instruct": "Tencent: Hunyuan A13B Instruct (131,072 tokens)",
                  "thedrummer/cydonia-24b-v4.1": "TheDrummer: Cydonia 24B V4.1 (131,072 tokens)",
                  "thedrummer/rocinante-12b": "TheDrummer: Rocinante 12B (32,768 tokens)",
                  "thedrummer/skyfall-36b-v2": "TheDrummer: Skyfall 36B V2 (32,768 tokens)",
                  "thedrummer/unslopnemo-12b": "TheDrummer: UnslopNemo 12B (32,768 tokens)",
                  "tngtech/deepseek-r1t-chimera": "TNG: DeepSeek R1T Chimera (163,840 tokens)",
                  "tngtech/deepseek-r1t-chimera:free": "TNG: DeepSeek R1T Chimera (free) (163,840 tokens)",
                  "tngtech/deepseek-r1t2-chimera": "TNG: DeepSeek R1T2 Chimera (163,840 tokens)",
                  "tngtech/deepseek-r1t2-chimera:free": "TNG: DeepSeek R1T2 Chimera (free) (163,840 tokens)",
                  "tngtech/tng-r1t-chimera": "TNG: R1T Chimera (163,840 tokens)",
                  "tngtech/tng-r1t-chimera:free": "TNG: R1T Chimera (free) (163,840 tokens)",
                  "undi95/remm-slerp-l2-13b": "ReMM SLERP 13B (6,144 tokens)",
                  "x-ai/grok-3": "xAI: Grok 3 (131,072 tokens)",
                  "x-ai/grok-3-beta": "xAI: Grok 3 Beta (131,072 tokens)",
                  "x-ai/grok-3-mini": "xAI: Grok 3 Mini (131,072 tokens)",
                  "x-ai/grok-3-mini-beta": "xAI: Grok 3 Mini Beta (131,072 tokens)",
                  "x-ai/grok-4": "xAI: Grok 4 (256,000 tokens)",
                  "x-ai/grok-4-fast": "xAI: Grok 4 Fast (2,000,000 tokens)",
                  "x-ai/grok-4.1-fast": "xAI: Grok 4.1 Fast (2,000,000 tokens)",
                  "x-ai/grok-code-fast-1": "xAI: Grok Code Fast 1 (256,000 tokens)",
                  "xiaomi/mimo-v2-flash": "Xiaomi: MiMo-V2-Flash (262,144 tokens)",
                  "xiaomi/mimo-v2-flash:free": "Xiaomi: MiMo-V2-Flash (free) (262,144 tokens)",
                  "z-ai/glm-4-32b": "Z.AI: GLM 4 32B  (128,000 tokens)",
                  "z-ai/glm-4.5": "Z.AI: GLM 4.5 (131,072 tokens)",
                  "z-ai/glm-4.5-air": "Z.AI: GLM 4.5 Air (131,072 tokens)",
                  "z-ai/glm-4.5-air:free": "Z.AI: GLM 4.5 Air (free) (131,072 tokens)",
                  "z-ai/glm-4.5v": "Z.AI: GLM 4.5V (65,536 tokens)",
                  "z-ai/glm-4.6": "Z.AI: GLM 4.6 (202,752 tokens)",
                  "z-ai/glm-4.6:exacto": "Z.AI: GLM 4.6 (exacto) (204,800 tokens)",
                  "z-ai/glm-4.6v": "Z.AI: GLM 4.6V (131,072 tokens)",
                  "z-ai/glm-4.7": "Z.AI: GLM 4.7 (202,752 tokens)",
                  "z-ai/glm-4.7-flash": "Z.AI: GLM 4.7 Flash (200,000 tokens)"
                },
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "google/gemini-2.5-flash"
              },
              "site_url": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "Site URL",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "site_url",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "SliderInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "max_label": "",
                "max_label_icon": "",
                "min_label": "",
                "min_label_icon": "",
                "name": "temperature",
                "override_skip": false,
                "placeholder": "",
                "range_spec": {
                  "max": 2,
                  "min": 0,
                  "step": 0.01,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "title_case": false,
                "tool_mode": false,
                "track_in_telemetry": false,
                "type": "slider",
                "value": 0.7
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "OpenRouterComponent"
        },
        "dragging": false,
        "id": "OpenRouterComponent-j1wSC",
        "measured": {
          "height": 483,
          "width": 320
        },
        "position": {
          "x": 1091.2720200322842,
          "y": 816.1503805357901
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": -824.8167984016002,
      "y": -943.2481562374153,
      "zoom": 1.3138900765806825
    }
  },
  "description": "Perform basic prompting with an OpenRouter model.",
  "endpoint_name": null,
  "id": "8bcdb301-e0ad-48b2-8c25-fe5925b73940",
  "is_component": false,
  "last_tested_version": "1.7.2",
  "name": "BUTTON2BEEHIIV.CHAT_WITH_USER",
  "tags": [
    "chatbots"
  ]
}